{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with PyTorch and BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is the same as in the word-embedding-raw file. But still not yet journal data!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.1+cu102'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400 1800 1800\n"
     ]
    }
   ],
   "source": [
    "size = \"small\"\n",
    "\n",
    "with open(f'nlpwdl2021_data/thedeep.{size}.train.txt', \"r\", encoding=\"utf8\") as csvfile:\n",
    "    train = list(csv.reader(csvfile)) # list of lists with 3 entries: sentence ID, text, label\n",
    "    \n",
    "with open(f'nlpwdl2021_data/thedeep.{size}.test.txt', \"r\", encoding=\"utf8\") as csvfile:\n",
    "    test = list(csv.reader(csvfile))\n",
    "    \n",
    "with open(f'nlpwdl2021_data/thedeep.{size}.validation.txt', \"r\", encoding=\"utf8\") as csvfile:\n",
    "    validation = list(csv.reader(csvfile))\n",
    "    \n",
    "print(len(train), len(test), len(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing and Dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text:str=None, return_label:bool=False, label:int=None):\n",
    "    \"\"\"function to preprocess string\n",
    "    returns a list of tokens\n",
    "    \"\"\"\n",
    "    #1 to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #2 remove all special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    \n",
    "    #3 remove single characters with space to the left and right (possessive pronoun)\n",
    "    text = re.sub(r\"\\s+[a-z]\\s+\", \" \", text)\n",
    "    \n",
    "    #4 remove double whitespaces to single space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    #5.2 replace numbers\n",
    "    text = re.sub(r'\\d+', '<num>', text)\n",
    "    \n",
    "    #5.3 now count frequncy of <dates> and <num>\n",
    "    count_num, count_dates = text.count(\"<num>\"), text.count(\"<dates>\")\n",
    "    \n",
    "    #5.4 replace it (so that in tokens it does not appear)\n",
    "    text = text.replace(\"<num>\", \"\").replace(\"<dates>\", \"\")\n",
    "    \n",
    "    #6 Stop words and tokenization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    token_text = [i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    #7 Lemmatization\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    result = [stemmer.lemmatize(word) for word in token_text]\n",
    "    if not return_label:\n",
    "        return result\n",
    "    else:\n",
    "        return result, label\n",
    "\n",
    "def create_dictionary(preprocessed_tokens:list=None):\n",
    "    \"\"\"Creates a word dictionary given a PREPROCESSED text. Returns a sorted dct of all counts and OOV as well as the count list\"\"\"\n",
    "    threshold = 2\n",
    "\n",
    "    count_list = list()\n",
    "    filtered_dict = dict()\n",
    "    out_of_vocabulary = list()\n",
    "    \n",
    "    word_dict = defaultdict(lambda: 0)\n",
    "    for word in preprocessed_tokens:\n",
    "        word_dict[word] += 1\n",
    "    \n",
    "    sorted_dict = {k: v for k, v in sorted(word_dict.items(), key=lambda item:item[1], reverse=True)}\n",
    "    \n",
    "    for key, value in sorted_dict.items():\n",
    "        if value > threshold:\n",
    "            filtered_dict[key] = value\n",
    "        else:\n",
    "            out_of_vocabulary.append(key)\n",
    "\n",
    "        count_list.append(value)\n",
    "    \n",
    "    return sorted_dict, out_of_vocabulary, count_list, list(filtered_dict.values())\n",
    "\n",
    "def merge_tokens(data:list):\n",
    "    \"\"\"\n",
    "    function that preprocesses text and returns two different lists\n",
    "    1. token_list ... used for dictionary to find out threshold value\n",
    "    2. documents ... used for removing oov words\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    token_list = list() # list with all tokens of all douments [\"word1\", \"word2\", ...]\n",
    "    documents = list() # elements are list of tokens [[doc1 tokens], [doc2 tokens], ...]\n",
    "    \n",
    "    for sample in tqdm(data):\n",
    "        token_list += preprocess_text(sample[1])\n",
    "        documents.append(preprocess_text(sample[1]))\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "    return token_list, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create the Dictionarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_labels_train = {} # contains all dictionarys of documtnts\n",
    "text_and_labels_test = {} \n",
    "text_and_labels_val = {} \n",
    "\n",
    "vocabulary = {}\n",
    "index = 0\n",
    "# get train set\n",
    "for doc in train:\n",
    "    # preprocess text, make dict and cut of tokens (see thresold from create_dictionary function)\n",
    "    pre_text = list(create_dictionary(preprocess_text(doc[1]))[0].keys())\n",
    "    \n",
    "    # add words to vocabulary (and numerate them) if they are not in there already\n",
    "    for word in pre_text:\n",
    "        if word not in vocabulary.keys():\n",
    "            vocabulary[word] = index\n",
    "            index += 1\n",
    "            \n",
    "    # since our padding must be in the range of vocabulary, we need to add it at the end of all words        \n",
    "    vocabulary[\"<PADDING_PLACEHOLDER>\"] = len(vocabulary) - 1\n",
    "    \n",
    "    # assign the id the label and (preprocessed) text, which is a dictionary with the respective threshold\n",
    "    text_and_labels_train[doc[0]] = {\"label\": doc[2], \"tokens\": pre_text}\n",
    "    \n",
    "    \n",
    "    \n",
    "# now with test set\n",
    "for doc in test:\n",
    "    # preprocess text, make dict and cut of tokens (see thresold from create_dictionary function)\n",
    "    pre_text = list(create_dictionary(preprocess_text(doc[1]))[0].keys())\n",
    "    \n",
    "    # assign the id the label and (preprocessed) text, which is a dictionary with the respective threshold\n",
    "    text_and_labels_test[doc[0]] = {\"label\": doc[2], \"tokens\": pre_text}\n",
    "    \n",
    "    \n",
    "# now with validation set\n",
    "for doc in validation:\n",
    "    # preprocess text, make dict and cut of tokens (see thresold from create_dictionary function)\n",
    "    pre_text = list(create_dictionary(preprocess_text(doc[1]))[0].keys())\n",
    "    \n",
    "    # assign the id the label and (preprocessed) text, which is a dictionary with the respective threshold\n",
    "    text_and_labels_val[doc[0]] = {\"label\": doc[2], \"tokens\": pre_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets stored into dictionarys of the following format:\n",
    "\n",
    "`{ID: {\"label\": <the label of the text>, \"tokens\": <tokens of the text>}}`\n",
    "\n",
    "Important: The tokens are already reduced: out of vocabulary words are removed as well as our cutoff/threshold we set in the function `create_dictionary()`.\n",
    "\n",
    "Also, the variable (dictionary) `vocabulary` now holds all the words in the `train` set, given a number (id) for the later processing into arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7084</td>\n",
       "      <td>11</td>\n",
       "      <td>[drew, close, south, india, combined, reservoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8829</td>\n",
       "      <td>3</td>\n",
       "      <td>[humanitarian, food, assistance, derna, supply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5295</td>\n",
       "      <td>3</td>\n",
       "      <td>[idp, jebel, marra, likely, humanitarian, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5619</td>\n",
       "      <td>4</td>\n",
       "      <td>[case, july, total, death, reported, currently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6717</td>\n",
       "      <td>5</td>\n",
       "      <td>[however, according, last, cadre, harmonis, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>696</td>\n",
       "      <td>11</td>\n",
       "      <td>[pokot, ka, resident, west, dire, need, food, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>733</td>\n",
       "      <td>4</td>\n",
       "      <td>[south, sudan, malaria, disease, people, toll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>132</td>\n",
       "      <td>9</td>\n",
       "      <td>[girl, prostitution, camp, engage, thing, ethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>6284</td>\n",
       "      <td>6</td>\n",
       "      <td>[trash, municipal, collection, service, derna,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>4224</td>\n",
       "      <td>6</td>\n",
       "      <td>[snowfall, resulted, road, blockage, affected,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id label                                             tokens\n",
       "0     7084    11  [drew, close, south, india, combined, reservoi...\n",
       "1     8829     3  [humanitarian, food, assistance, derna, supply...\n",
       "2     5295     3  [idp, jebel, marra, likely, humanitarian, with...\n",
       "3     5619     4  [case, july, total, death, reported, currently...\n",
       "4     6717     5  [however, according, last, cadre, harmonis, an...\n",
       "...    ...   ...                                                ...\n",
       "1795   696    11  [pokot, ka, resident, west, dire, need, food, ...\n",
       "1796   733     4  [south, sudan, malaria, disease, people, toll,...\n",
       "1797   132     9  [girl, prostitution, camp, engage, thing, ethi...\n",
       "1798  6284     6  [trash, municipal, collection, service, derna,...\n",
       "1799  4224     6  [snowfall, resulted, road, blockage, affected,...\n",
       "\n",
       "[1800 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20  \n",
    "max_len = 50  # where to cut off (see datalaoder method)\n",
    "\n",
    "# convert our dicts to panda dataframes\n",
    "train_dataframe = pd.DataFrame.from_dict(text_and_labels_train,orient='index')\n",
    "train_dataframe.index.name = 'id'\n",
    "train_dataframe.reset_index(inplace=True)\n",
    "\n",
    "test_dataframe = pd.DataFrame.from_dict(text_and_labels_test,orient='index')\n",
    "test_dataframe.index.name = 'id'\n",
    "test_dataframe.reset_index(inplace=True)\n",
    "\n",
    "val_dataframe = pd.DataFrame.from_dict(text_and_labels_val,orient='index')\n",
    "val_dataframe.index.name = 'id'\n",
    "val_dataframe.reset_index(inplace=True)\n",
    "\n",
    "val_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWordDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = [vocabulary.get(t) for t in self.data[\"tokens\"][idx][:max_len] if t is not None]\n",
    "        try:\n",
    "            tokens = np.array(tokens, dtype=np.int64)\n",
    "        except:\n",
    "            # in some cases there is None in the array for some reason \n",
    "            tokens = np.array(tokens)\n",
    "            tokens = tokens[tokens != np.array(None)]\n",
    "            # now after filtering None out, make an araay as before \n",
    "            tokens = np.array(tokens, dtype=np.int64)\n",
    "        \n",
    "        # now lets define a mask, to later feed the network with it\n",
    "        m = np.pad(np.ones_like(tokens, dtype=bool), (0, max_len - len(tokens)))\n",
    "        \n",
    "        # finally, pad the array to make sure they all have equal size!\n",
    "        tokens = np.pad(tokens, (0, max_len - len(tokens)),  constant_values=vocabulary[\"<PADDING_PLACEHOLDER>\"] - 1) \n",
    "        \n",
    "        # now return the data itself, the mask, and the labels:\n",
    "        return torch.from_numpy(tokens), torch.from_numpy(m), self.data[\"label\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CustomWordDataset(train_dataframe)\n",
    "test = CustomWordDataset(test_dataframe)\n",
    "validation = CustomWordDataset(val_dataframe)\n",
    "\n",
    "# finally, define the pytorch Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Word embedding lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load(\"word2vec-google-news-300\")\n",
    "vecs = wv.get_normed_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dictionary `token_embeds`. We will go over all tokens in the `vocabulary`, check if the token is already in the `token_embeds` and if, skip the token. At the end, `token_embeds` will have the structure `{\"word\": <word2vec embedding>, \"word2\": ....}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 18575 total tokens, 6650 tokens were not found (35.8%).\n",
      "18573\n"
     ]
    }
   ],
   "source": [
    "not_found = 0\n",
    "token_embeds = {}\n",
    "\n",
    "for token in vocabulary:\n",
    "    if token in token_embeds:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        token_embeds[token] = wv[token]\n",
    "        \n",
    "    except KeyError:\n",
    "        # random embedding\n",
    "        token_embeds[token] = np.random.uniform(low=-1.0, high=1.0, size=(vecs.shape[1]))\n",
    "        not_found += 1\n",
    "\n",
    "print(f\"Out of {len(vocabulary)} total tokens, {not_found} tokens were not found ({round(not_found/len(vocabulary), 4)*100}%).\")\n",
    "#print(list(token_embeds.items())[:1])\n",
    "print(vocabulary[\"<PADDING_PLACEHOLDER>\"] - 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18575, 300)\n"
     ]
    }
   ],
   "source": [
    "# convert to matrix\n",
    "wordemb_matrix = np.array([v for k, v in token_embeds.items()])\n",
    "print(wordemb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model definition + forward function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu.\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Training on {device}.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationAverageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(\n",
    "            wordemb_matrix.shape[0], wordemb_matrix.shape[1],\n",
    "            # use the current vectors as weight\n",
    "            _weight=torch.from_numpy(wordemb_matrix).float(),\n",
    "            # We need to use the padding placeholder (must be in vocabulary)\n",
    "            padding_idx=vocabulary[\"<PADDING_PLACEHOLDER>\"] - 1 \n",
    "        )\n",
    "    \n",
    "        # add one linear layer to get the output (We have 12 Labels, see labels.txt)\n",
    "        self.out = nn.Linear(wordemb_matrix.shape[1], 12)\n",
    "        \n",
    "    def forward(self, tokens:np.array=None, mask:np.array=None):\n",
    "        \"\"\"Simple forward function, we use the given formula but now in the network\"\"\"\n",
    "        nnembeding = self.emb(tokens).sum(1) / mask.sum(1, keepdims=True)\n",
    "                \n",
    "        return self.out(nnembeding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss function + Optimization + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationAverageModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0009)\n",
    "early_stop_epsilon = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader:torch.utils.data.DataLoader=None, model=None):\n",
    "    \"\"\"Returns accuracy given datalaoder and the model we have so far. Very similar as in the train loop.\"\"\"\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in dataloader:\n",
    "            batch = [t.to(device) for t in batch[:2]] + [np.array(batch[2], dtype=int)]\n",
    "        \n",
    "            # labels are not a tensor yet - make them one\n",
    "            tokens, mask, labels = batch\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            \n",
    "            # feed our model the tokens and the mask.\n",
    "            predictions = model(tokens, mask)\n",
    "            \n",
    "            # detach from cpu and store them in our list to later calcualte the accuracy\n",
    "            all_predictions.append(predictions.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "\n",
    "        # finaly calcualte accuracy given datalaoder\n",
    "        X = torch.argmax(torch.cat(all_predictions), -1)\n",
    "        y = torch.cat(all_labels)\n",
    "        \n",
    "        # we just look how many items are correctly predicted (we create a array of true false), and then take the mean from the items\n",
    "        return (X == y).to(torch.float32).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Accuracy (validation set): 48.0%\n",
      "Epoch: 1 | Accuracy (validation set): 56.00000000000001%\n",
      "Epoch: 2 | Accuracy (validation set): 60.0%\n",
      "Epoch: 3 | Accuracy (validation set): 62.0%\n",
      "Epoch: 4 | Accuracy (validation set): 62.0%\n",
      "No improvement detected (Epoch 4, tolerance=0.005). Early stopping...\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "best_acc = 0\n",
    "improvement = 0\n",
    "all_losses = []\n",
    "prev_validation_accuracy = 999\n",
    "\n",
    "for epoch in torch.arange(epochs):\n",
    "    loss_list = np.zeros((len(train_loader)), dtype=np.float64)\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        batch = [t.to(device) for t in batch[:2]] + [np.array(batch[2], dtype=int)]\n",
    "        \n",
    "        # labels are not a tensor yet - make them one\n",
    "        tokens, mask, labels = batch\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        \n",
    "        # since we are training \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the predictions\n",
    "        predictions = model(tokens, mask)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # add loss o list\n",
    "        loss_list[idx] = loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation time! \n",
    "    validation_accuracy = validation(validation_loader, model)\n",
    "    print(f\"Epoch: {epoch} | Accuracy (validation set): {round(validation_accuracy, 2)*100}%\")\n",
    "    \n",
    "    # add losses to list\n",
    "    all_losses.append(loss_list)\n",
    "    \n",
    "    if not int(epoch) == 0:\n",
    "        # we compare the val accuracy to the one from prev epoch (but skip the first, since theres nothing to compare).\n",
    "        if abs(validation_accuracy - prev_validation_accuracy) < early_stop_epsilon:\n",
    "            print(f\"No improvement detected (Epoch {epoch}, tolerance={early_stop_epsilon}). Early stopping...\")\n",
    "            break\n",
    "            \n",
    "    prev_validation_accuracy = validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test Set Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 63.8333%\n"
     ]
    }
   ],
   "source": [
    "# Now try our model on the test set:\n",
    "accuracy = validation(test_loader, model)\n",
    "print(f\"Final Accuracy: {round(accuracy*100, 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6615</td>\n",
       "      <td>Cholera Daily Situation Report as of 4 Novembe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>659</td>\n",
       "      <td>12 321 people affected, five deaths, one perso...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8591</td>\n",
       "      <td>Violent clashes and inter-communal tensions ha...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8373</td>\n",
       "      <td>AT least 12 people have been killed and severa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10125</td>\n",
       "      <td>Unidentified gunmen attacked a civilian home, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  label\n",
       "0   6615  Cholera Daily Situation Report as of 4 Novembe...      4\n",
       "1    659  12 321 people affected, five deaths, one perso...     10\n",
       "2   8591  Violent clashes and inter-communal tensions ha...      3\n",
       "3   8373  AT least 12 people have been killed and severa...      5\n",
       "4  10125  Unidentified gunmen attacked a civilian home, ...      9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = \"small\"\n",
    "\n",
    "df_train = pd.read_csv(f'nlpwdl2021_data/thedeep.{size}.train.txt', names=[\"id\", \"text\", \"label\"])\n",
    "df_val = pd.read_csv(f'nlpwdl2021_data/thedeep.{size}.validation.txt', names=[\"id\", \"text\", \"label\"])\n",
    "df_test = pd.read_csv(f'nlpwdl2021_data/thedeep.{size}.test.txt', names=[\"id\", \"text\", \"label\"])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e4c06cab1d43d993ba97c7ef359877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073323d33387491ca57cb7a07dbd14ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class ClassificationBERTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 12)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # _: embedding vectors, pooled_output: embedding vectors of [CLS] token\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "    \n",
    "model = ClassificationBERTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c998d2ae1264eefae7afa2ddc5e186f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06abdae7abf94d8eaef8a4e2efe3c8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3961d7eb854cb094abbd75c11484dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = list(df['label'])\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    # get train data and val data\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    # prepare dataloaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=4)\n",
    "    # gpu preferred with bert\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # declare loss function where softmax is included in this function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # choose optimizer\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    # training time\n",
    "    for epoch_num in range(epochs):\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            # training set \n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                # put ur stuff on the GPU\n",
    "                train_label = torch.tensor(train_label, dtype=torch.long, device=device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                output = model(input_id, mask)\n",
    "                # calculate soft-probabilities and negLogLikeLoss \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "                # set gradients to zero\n",
    "                model.zero_grad()\n",
    "                # compute dloss/dx for every parameter\n",
    "                batch_loss.backward()\n",
    "                # performs a parameter update based on the current gradient\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # validation set\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = torch.tensor(val_label, dtype=torch.long, device=device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2100 [00:00<?, ?it/s]/tmp/ipykernel_7680/3037234860.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_label = torch.tensor(train_label, dtype=torch.long, device=device)\n",
      "100%|███████████████████████████████████████| 2100/2100 [11:04<00:00,  3.16it/s]\n",
      "/tmp/ipykernel_7680/3037234860.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_label = torch.tensor(val_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.377 | Train Accuracy:  0.592 | Val Loss:  0.358 | Val Accuracy:  0.610\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = torch.tensor(test_label, dtype=torch.long, device=device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7680/2444823547.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_label = torch.tensor(test_label, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.627\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses = []\n",
    "for l in all_losses:\n",
    "    mean_losses.append(l.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mean_losses)\n",
    "plt.title(\"Loss while training\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

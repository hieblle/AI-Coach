{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The datasets are taken from the data of `thedeep` project, produced by the DEEP (https://www.thedeep.io) platform. The DEEP is an open-source platform, which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset has 12 classes (labels) like agriculture, health, and protection. \n",
    "\n",
    "Download from [this link](https://drive.jku.at/filr/public-link/file-download/0cce88f07c9c862b017c9cfba294077a/33590/5792942781153185740/nlp2021_22_data.zip).\n",
    "\n",
    "- `thedeep.$name$.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.label.txt`: Captions of the labels.\n",
    "- `README.txt`: Terms of use of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity, Nearest Neighbors, and WE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate word-to-word similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the word-embedder model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_calc(vec_1,vec_2):\n",
    "\tsim = np.dot(vec_1,vec_2) / (np.linalg.norm(vec_1)*np.linalg.norm(vec_2))\n",
    "\treturn sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_to_words(source_word:str, compare:list):\n",
    "    source_vec = wv[source_word]\n",
    "    for word in compare:\n",
    "        comp_vec = wv[word]\n",
    "        print(f\"cosine sim({source_word}, {word}):\", cosine_similarity_calc(source_vec, comp_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim(car, minivan): 0.69070363\n",
      "cosine sim(car, bicycle): 0.5364484\n",
      "cosine sim(car, airplane): 0.42435578\n",
      "cosine sim(car, cereal): 0.13924746\n",
      "cosine sim(car, communism): 0.05820294\n"
     ]
    }
   ],
   "source": [
    "source_1 = \"car\"\n",
    "compare_list1 = [\"minivan\",\"bicycle\",\"airplane\",\"cereal\", \"communism\"]\n",
    "source_to_words(source_1, compare_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim(fish, shark): 0.5278309\n",
      "cosine sim(fish, sea): 0.3250058\n",
      "cosine sim(fish, fishing): 0.63979906\n",
      "cosine sim(fish, water): 0.42041764\n",
      "cosine sim(fish, boat): 0.37478778\n"
     ]
    }
   ],
   "source": [
    "source_2 = \"fish\"\n",
    "compare_list2 = [\"shark\",\"sea\",\"fishing\",\"water\", \"boat\"]\n",
    "source_to_words(source_2, compare_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim(rice, asia): 0.18458173\n",
      "cosine sim(rice, bowl): 0.25551498\n",
      "cosine sim(rice, plant): 0.13637118\n",
      "cosine sim(rice, China): 0.17849869\n",
      "cosine sim(rice, sushi): 0.36377826\n"
     ]
    }
   ],
   "source": [
    "source_3 = \"rice\"\n",
    "compare_list3 = [\"asia\",\"bowl\",\"plant\",\"China\", \"sushi\"]\n",
    "source_to_words(source_3, compare_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(source_vec, target_vec, k):\n",
    "    \"\"\"\n",
    "    efficient way: dot product between vector and matrix produces\n",
    "    list with scalars where each entry is divided by \n",
    "    its corresponding norm vector\n",
    "    ----------\n",
    "    returns [(\"most similar word\", cosine_value), ...]\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    # create list where each element is norm of row vector\n",
    "    vec_norms = [np.linalg.norm(vec) for vec in target_vec]\n",
    "    source_len = np.linalg.norm(source_vec)\n",
    "\n",
    "    # our model has shope 3mill x 300, but we need 300 x 3mill,\n",
    "    # so we get a list with scalars \n",
    "    target_transposed = target_vec.T\n",
    "    cos_list = np.dot(source_vec, target_transposed)\n",
    "    \n",
    "    # we divide list of scalars with (||source_vec|| * ||target vec||   \n",
    "    for idx, vec_len in enumerate(vec_norms):\n",
    "        cos_list[idx] = cos_list[idx] / (source_len * vec_len)\n",
    "        \n",
    "    k_indices = np.argpartition(cos_list, -k)[-k:] # get indices of k largest elements\n",
    "    \n",
    "    to_sort = list()\n",
    "    for idx in k_indices:\n",
    "        to_sort.append((wv.index_to_key[idx], round(cos_list[idx], 4)))\n",
    "        \n",
    "    sorted_k = sorted(to_sort, key = lambda x: x[1])[::-1] # sort by second element and reverse list\n",
    "    return sorted_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rice', 1.0),\n",
       " ('milled_rice', 0.6632),\n",
       " ('wheat_flour', 0.6618),\n",
       " ('paddy_rice', 0.6603),\n",
       " ('paddy', 0.6452),\n",
       " ('unhusked_rice', 0.6451),\n",
       " ('cassava', 0.6379),\n",
       " ('parboiled_rice', 0.637),\n",
       " ('maize', 0.636),\n",
       " ('wheat', 0.6308)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(wv[\"rice\"], wv.vectors, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mathematics', 1.0),\n",
       " ('math', 0.8161),\n",
       " ('Mathematics', 0.7574),\n",
       " ('maths', 0.742),\n",
       " ('Math', 0.6675),\n",
       " ('mathematics_physics', 0.6631),\n",
       " ('algebra_trigonometry', 0.656),\n",
       " ('calculus_trigonometry', 0.6443),\n",
       " ('algebra', 0.6425),\n",
       " ('mathematical_sciences', 0.6379)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(wv[\"mathematics\"], wv.vectors, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dildo', 1.0),\n",
       " ('dildos', 0.6557),\n",
       " ('vibrator', 0.6179),\n",
       " ('nipple_clamps', 0.603),\n",
       " ('strap_ons', 0.5892),\n",
       " ('clit', 0.5726),\n",
       " ('vagina', 0.5708),\n",
       " ('vibrators', 0.5669),\n",
       " ('anally', 0.5658),\n",
       " ('dick', 0.5645)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors(wv[\"dildo\"], wv.vectors, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WE evaluation\n",
    "***\n",
    "1) Word Similarity:\n",
    "- datasets: WordSim353 paritioned into two datasets, WordSim Similarity, WordSim Relatedness, MEN dataset, Mechanical Turk dataset, Rare Words dataset, SimLex-999 dataset\n",
    "- The word vectors are evaluated by ranking the pairs according to their cosine similarities, and measuring the correlation (Spearman’s ρ) with the human ratings\n",
    "\n",
    "2) Analogy:\n",
    "- datasets: MSR analogy dataset, Google analogy dataset\n",
    "- the two analogy datasets present questions of the form \"a is to a as b is to b*\" where b* is hidden, and must be guessed from the entire vocabulary\n",
    "- analogy questions are answered using 3CosAdd and 3CosMul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_words = wv.index_to_key\n",
    "m_turk = list()\n",
    "m_turk_similarities = list()\n",
    "\n",
    "with open(\"Mtruk.csv\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    for row in csv_reader:\n",
    "        # do not include words that are not in our embedding model\n",
    "        if row[0] not in wv_words or row[1] not in wv_words:\n",
    "            continue\n",
    "        else:\n",
    "            m_turk.append(row) # list of lists\n",
    "            m_turk_similarities.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_similarities = list()\n",
    "\n",
    "for elem in m_turk:\n",
    "    wv_similarities.append(cosine_similarity_calc(wv[elem[0]], wv[elem[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6843994695942136, pvalue=2.44586235254813e-39)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(wv_similarities, m_turk_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_cos_add(a, a_star, b, wv):\n",
    "    a_index, a_star_index = wv.get_index(a), wv.get_index(a_star)\n",
    "    b_index = wv.get_index(b)\n",
    "    wv_normed = wv.get_normed_vectors()\n",
    "    sim_list = cosine_similarity_calc(wv_normed, wv[a_star] - wv[a] + wv[b])\n",
    "    # exclude a, a_star and b from embedding model\n",
    "    sim_list[np.array([a_index, a_star_index, b_index])] = -np.inf\n",
    "    return np.argmax(sim_list)\n",
    "\n",
    "def three_cos_mul(a, a_star, b, wv, epsilon=1e-3):\n",
    "    a_index, a_star_index = wv.get_index(a), wv.get_index(a_star)\n",
    "    b_index = wv.get_index(b)\n",
    "    wv_normed = wv.get_normed_vectors()\n",
    "    sim_list = (cosine_similarity_calc(wv_normed, wv[a_star]) * cosine_similarity_calc(wv_normed, wv[b])) / (cosine_similarity_calc(wv_normed, wv[a]) + epsilon)\n",
    "    # exclude a, a_star and b from embedding model\n",
    "    sim_list[np.array([a_index, a_star_index, b_index])] = -np.inf\n",
    "    return np.argmax(sim_list)\n",
    "\n",
    "def find_analogy(a:str, a_star:str, b:str, embedding_model:list, method):\n",
    "    \"\"\"\n",
    "    find b_star such that a is to a_star as b is to b_star\n",
    "    -------------------------\n",
    "    returns most analogical word as string\n",
    "    -------------------------\n",
    "    \"\"\"\n",
    "    return wv.index_to_key[method(a, a_star, b, embedding_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogy('man',\"king\",\"woman\", wv, three_cos_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogy dataset\n",
    "with open(\"questions-words.txt\") as fh:\n",
    "    text_list = fh.read().splitlines()[1:] # elements are strings\n",
    "    text_list = [elem.split() for elem in text_list] # split by whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of analogies with smaller subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_analogies(test_set:list, method):\n",
    "    n = len(test_set)\n",
    "    correct = 0\n",
    "    \n",
    "    for elem in tqdm(test_set, desc=\"evaluate analogies\"):\n",
    "        analogy = find_analogy(elem[0], elem[1], elem[2], wv, method)\n",
    "        if analogy == elem[3]:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "evaluate analogies: 100%|██████████████████████████████████████████████████████████| 1000/1000 [24:51<00:00,  1.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.753"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "evaluate_analogies(np.random.choice(text_list, 1000), three_cos_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "evaluate analogies: 100%|██████████████████████████████████████████████████████████| 1000/1000 [44:11<00:00,  2.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.562"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "evaluate_analogies(np.random.choice(text_list, 1000), three_cos_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cos_mul depends here on the seed, other experiment runs output a range from 50 to 75 percent, running this function took a lot of time so we omitted further experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification with WE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400 1800 1800\n"
     ]
    }
   ],
   "source": [
    "size = \"small\"\n",
    "\n",
    "with open(f'nlpwdl2021_data/thedeep.{size}.train.txt', \"r\", encoding=\"utf8\") as csvfile:\n",
    "    train = list(csv.reader(csvfile)) # list of lists with 3 entries: sentence ID, text, label\n",
    "    \n",
    "with open(f'nlpwdl2021_data/thedeep.{size}.test.txt', \"r\", encoding=\"utf8\") as csvfile:\n",
    "    test = list(csv.reader(csvfile))\n",
    "    \n",
    "with open(f'nlpwdl2021_data/thedeep.{size}.validation.txt', \"r\", encoding=\"utf8\") as csvfile:\n",
    "    validation = list(csv.reader(csvfile))\n",
    "    \n",
    "print(len(train), len(test), len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse old functions\n",
    "def preprocess_text(text:str=None, return_label:bool=False, label:int=None):\n",
    "    \"\"\"function to preprocess string\n",
    "    returns a list of tokens\n",
    "    \"\"\"\n",
    "    #1 to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #2 remove all special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    \n",
    "    #3 remove single characters with space to the left and right (possessive pronoun)\n",
    "    text = re.sub(r\"\\s+[a-z]\\s+\", \" \", text)\n",
    "    \n",
    "    #4 remove double whitespaces to single space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    #5.2 replace numbers\n",
    "    text = re.sub(r'\\d+', '<num>', text)\n",
    "    \n",
    "    #5.3 now count frequncy of <dates> and <num>\n",
    "    count_num, count_dates = text.count(\"<num>\"), text.count(\"<dates>\")\n",
    "    \n",
    "    #5.4 replace it (so that in tokens it does not appear)\n",
    "    text = text.replace(\"<num>\", \"\").replace(\"<dates>\", \"\")\n",
    "    \n",
    "    #6 Stop words and tokenization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    token_text = [i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    #7 Lemmatization\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    result = [stemmer.lemmatize(word) for word in token_text]\n",
    "    if not return_label:\n",
    "        return result\n",
    "    else:\n",
    "        return result, label\n",
    "\n",
    "def create_dictionary(preprocessed_tokens:list=None):\n",
    "    \"\"\"Creates a word dictionary given a PREPROCESSED text. Returns a sorted dct of all counts and OOV as well as the count list\"\"\"\n",
    "    threshold = 2\n",
    "\n",
    "    count_list = list()\n",
    "    filtered_dict = dict()\n",
    "    out_of_vocabulary = list()\n",
    "    \n",
    "    word_dict = defaultdict(lambda: 0)\n",
    "    for word in preprocessed_tokens:\n",
    "        word_dict[word] += 1\n",
    "    \n",
    "    sorted_dict = {k: v for k, v in sorted(word_dict.items(), key=lambda item:item[1], reverse=True)}\n",
    "    \n",
    "    for key, value in sorted_dict.items():\n",
    "        if value > threshold:\n",
    "            filtered_dict[key] = value\n",
    "        else:\n",
    "            out_of_vocabulary.append(key)\n",
    "\n",
    "        count_list.append(value)\n",
    "    \n",
    "    return sorted_dict, out_of_vocabulary, count_list\n",
    "\n",
    "def merge_tokens(data:list):\n",
    "    \"\"\"\n",
    "    function that preprocesses text and returns two different lists\n",
    "    1. token_list ... used for dictionary to find out threshold value\n",
    "    2. documents ... used for removing oov words\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    token_list = list() # list with all tokens of all douments [\"word1\", \"word2\", ...]\n",
    "    documents = list() # elements are list of tokens [[doc1 tokens], [doc2 tokens], ...]\n",
    "    \n",
    "    for sample in tqdm(data):\n",
    "        token_list += preprocess_text(sample[1])\n",
    "        documents.append(preprocess_text(sample[1]))\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "    return token_list, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the words to embeddings\n",
    "The model used is the same as abve; its from google Word2Vec - we map the words from the document (after preprocessing and cuting the threshould words out) to the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train -----------------------\n",
    "# preprocess all texts\n",
    "texts_train = []\n",
    "for doc in train:\n",
    "    texts_train.append(preprocess_text(doc[1]))\n",
    "\n",
    "# make a dictionary\n",
    "dictionarys_texts_list_train = [] # contains all dictionarys\n",
    "for pre_text in texts_train:\n",
    "    dictionarys_texts_list_train.append(create_dictionary(pre_text)[0])\n",
    "    \n",
    "    \n",
    "# for test ----------------------\n",
    "texts_test = []\n",
    "for doc in test:\n",
    "    texts_test.append(preprocess_text(doc[1]))\n",
    "\n",
    "# make a dictionary\n",
    "dictionarys_texts_list_test = [] # contains all dictionarys\n",
    "for pre_text in texts_test:\n",
    "    dictionarys_texts_list_train.append(create_dictionary(pre_text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go over every dictionary in the list (dictionary_text_list) and map them with the pre trained model\n",
    "word_map_train = {} # maps word to embedding\n",
    "word_map_test = {}\n",
    "not_found = 0\n",
    "\n",
    "# for train\n",
    "for doc in dictionarys_texts_list_train:\n",
    "    for word in doc:\n",
    "        if word not in word_map_train.keys():\n",
    "            try:\n",
    "                word_map_train[word] = np.array(wv[word])\n",
    "            except KeyError:\n",
    "                # choose random embedding (from already existing ones)\n",
    "                word_map_train[word] = np.random.uniform(low=-1.0, high=1.0, size=(list(word_map_train.values())[0]).shape)\n",
    "                not_found += 1\n",
    "                \n",
    "                \n",
    "# for test \n",
    "for doc in dictionarys_texts_list_test:\n",
    "    for word in doc:\n",
    "        if word not in word_map_test.keys():\n",
    "            try:\n",
    "                word_map_test[word] = np.array(wv[word])\n",
    "            except KeyError:\n",
    "                # choose random embedding (from already existing ones)\n",
    "                word_map_test[word] = np.random.uniform(low=-1.0, high=1.0, size=(list(word_map_test.values())[0]).shape)\n",
    "                not_found += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_representation(doc:list=None):\n",
    "    \"\"\"Input: doc, a list of vectors, same shape.\n",
    "    Returns the value of above formula\"\"\"\n",
    "    \n",
    "    # add vectors vertically\n",
    "    e_d = (1/len(doc)) * np.sum(doc, axis=0)\n",
    "    \n",
    "    return e_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Classify the documants from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_and_labels_train = {} # contains all dictionarys\n",
    "text_and_labels_test = {} # contains all dictionarys\n",
    "\n",
    "\n",
    "# get train set\n",
    "for doc in train:\n",
    "    # preprocess text, make dict and cut of tokens (see thresold from create_dictionary function)\n",
    "    pre_text = list(create_dictionary(preprocess_text(doc[1]))[0].keys())\n",
    "    \n",
    "    # assign the id the label and (preprocessed) text, which is a dictionary with the respective threshold\n",
    "    text_and_labels_train[doc[0]] = {\"label\": doc[2], \"text\": pre_text}\n",
    "    \n",
    "    # now add a new entry - the vector representation e_v (Exercise abouve)\n",
    "    text_and_labels_train[doc[0]][\"vector\"] = doc_representation([word_map_train[word] for word in pre_text])\n",
    "    \n",
    "# now with test set\n",
    "for doc in test:\n",
    "    # preprocess text, make dict and cut of tokens (see thresold from create_dictionary function)\n",
    "    pre_text = list(create_dictionary(preprocess_text(doc[1]))[0].keys())\n",
    "    \n",
    "    # assign the id the label and (preprocessed) text, which is a dictionary with the respective threshold\n",
    "    text_and_labels_test[doc[0]] = {\"label\": doc[2], \"text\": pre_text}\n",
    "    \n",
    "    # now add a new entry - the vector representation e_v (Exercise abouve)\n",
    "    text_and_labels_test[doc[0]][\"vector\"] = doc_representation([word_map_train[word] for word in pre_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('11267', {'label': '4', 'text': ['health', 'said', 'died', 'facility', 'minister', 'jonglei', 'state', 'angok', 'gordon', 'far', 'people', 'cholera', 'duk', 'county', 'number', 'reported', 'last', 'evening', 'actually', 'case', 'monday', 'april', 'added', 'still', 'treated', 'rest', 'discharged'], 'vector': array([-2.40211193e-02, -2.69282547e-03,  4.65271574e-02,  4.16446081e-02,\n",
      "       -4.11767683e-02, -2.52486798e-02,  5.06995292e-02, -7.73757420e-02,\n",
      "        3.09039325e-02, -3.39679409e-02, -3.14135742e-02, -1.81439759e-01,\n",
      "       -5.62723502e-02,  3.47959475e-02, -6.61549626e-02,  9.49345343e-02,\n",
      "       -2.53688379e-03,  8.95255864e-02, -4.16840290e-02,  8.16017567e-05,\n",
      "       -3.48079483e-02,  4.07584960e-02,  4.57891639e-02, -6.12057694e-02,\n",
      "        6.63169626e-02, -3.12747544e-02, -6.72252837e-02,  1.02127287e-02,\n",
      "       -3.27678302e-02, -9.11770967e-03,  3.09081945e-02, -1.34938124e-02,\n",
      "       -8.17382915e-02, -7.44070315e-02, -2.20341461e-02, -6.52211166e-03,\n",
      "       -3.91954889e-02, -3.00625008e-03,  7.15526850e-03,  5.01175485e-02,\n",
      "        3.21413948e-04, -5.74376672e-02,  6.73751901e-02,  2.80672071e-02,\n",
      "        5.27126271e-02, -8.20227721e-02, -7.28875499e-02, -2.19631216e-02,\n",
      "       -2.59513694e-02,  3.06014657e-02,  5.72300861e-04, -8.14591363e-02,\n",
      "        1.56529295e-02, -1.14388546e-02, -6.69700746e-02, -4.60573697e-02,\n",
      "       -9.04969182e-02,  1.50087163e-02,  8.08592839e-02, -6.48733758e-02,\n",
      "       -1.00378774e-01,  8.29457581e-02, -3.55628859e-02, -3.37726103e-02,\n",
      "        1.99164159e-02, -3.43873294e-02, -1.11790793e-02,  2.30515193e-02,\n",
      "       -1.25912284e-01,  7.34955027e-02,  1.27480126e-03,  2.60167557e-02,\n",
      "        1.53958376e-01,  1.57220570e-02, -5.67906041e-02,  5.30022113e-02,\n",
      "        6.14986420e-02, -2.42867611e-02,  3.37911911e-02,  5.17779990e-02,\n",
      "       -5.68042800e-03, -4.55614700e-02, -1.78644008e-02, -4.19587722e-03,\n",
      "       -8.30473932e-02, -4.82022016e-03, -1.36251314e-01,  9.03082775e-02,\n",
      "        3.53292405e-02, -6.71365995e-02,  1.18208806e-01, -5.40172885e-03,\n",
      "       -4.16927556e-02, -4.37239731e-02, -7.90370205e-02,  3.46472279e-02,\n",
      "        4.95033718e-02,  5.73204737e-02,  5.96912331e-02, -1.05498879e-01,\n",
      "       -3.80517900e-02,  3.85845945e-03, -5.82617266e-02, -9.95559611e-03,\n",
      "       -1.17039189e-02, -3.27042193e-02, -4.94374699e-02,  5.44846047e-03,\n",
      "        3.42584835e-02, -9.51820560e-02, -1.94810246e-02, -1.13016766e-02,\n",
      "        3.54399401e-02, -3.11124202e-02,  7.51468049e-03,  3.36028689e-03,\n",
      "        2.57645676e-02, -1.17021352e-01,  5.82092555e-02,  4.69049729e-02,\n",
      "       -5.23181031e-02,  6.62020313e-02, -5.11082266e-02, -2.21949128e-02,\n",
      "       -1.02537976e-02,  2.96334036e-02,  1.55674176e-03, -7.62148238e-02,\n",
      "       -9.69470676e-03, -4.46678000e-02,  2.41831437e-02, -9.37871359e-02,\n",
      "       -2.76613955e-02, -7.36088135e-02, -7.11064253e-02, -4.43848990e-02,\n",
      "        1.44495014e-02,  2.76043976e-03,  1.00959082e-01,  1.33053421e-01,\n",
      "       -1.09787947e-02, -1.63167017e-02,  1.51018946e-02,  2.91771033e-02,\n",
      "       -4.67743838e-02,  6.20739543e-02, -7.43274059e-02, -9.45225782e-02,\n",
      "       -6.14417728e-02, -1.12187759e-01,  1.44207458e-01, -3.71254022e-02,\n",
      "       -1.02409786e-01,  8.86230537e-02, -1.28703480e-02, -8.49086827e-02,\n",
      "        4.15021035e-02, -8.30324252e-02, -5.41348356e-02,  1.12711987e-02,\n",
      "       -1.77869299e-02,  7.47594755e-02,  8.21344228e-02,  3.84590752e-03,\n",
      "        1.35479488e-02, -1.01510334e-01,  7.02664236e-02, -1.37113336e-02,\n",
      "       -2.71298115e-02,  3.99348043e-02, -7.78907810e-02,  2.64213497e-03,\n",
      "       -2.82851989e-02, -6.10836457e-02, -3.43670247e-02, -5.53637000e-02,\n",
      "        9.65263190e-02, -1.13264751e-01, -5.96250065e-02, -2.53978024e-02,\n",
      "       -3.83681094e-02,  4.70710792e-02, -4.56277344e-02,  1.76780366e-02,\n",
      "        4.67193446e-02,  4.03487387e-02, -9.15895946e-02,  4.69586667e-02,\n",
      "       -2.82625899e-02,  1.13281333e-01,  1.37438353e-02, -4.25208486e-02,\n",
      "       -5.20096226e-02,  8.43083401e-02, -1.66254756e-02,  9.96560629e-02,\n",
      "       -4.94829790e-02,  4.21395303e-02,  1.34435201e-02, -4.54150833e-02,\n",
      "       -1.86776778e-02,  5.32311633e-02, -6.32729837e-02,  9.94950792e-03,\n",
      "       -1.52690070e-02, -3.60322761e-02,  3.09836260e-02,  7.52478304e-03,\n",
      "        3.37574091e-02, -3.80986641e-02, -5.05977626e-02,  4.55936064e-02,\n",
      "       -4.51624683e-02,  6.16572520e-03, -9.61476898e-02, -5.22620442e-02,\n",
      "        3.24986807e-02,  3.86645615e-02, -8.50692236e-02,  4.77946891e-02,\n",
      "        4.34164041e-02,  1.30539976e-02,  6.07062217e-02,  6.17141929e-02,\n",
      "        9.86839341e-02, -9.45224914e-02,  1.65287611e-02,  1.25266534e-01,\n",
      "       -3.38774000e-03,  1.58164989e-02, -7.21589069e-04,  3.10970273e-02,\n",
      "       -1.29455232e-02, -3.03858627e-02, -4.85076275e-02, -2.64226932e-02,\n",
      "        6.91202026e-02,  3.15629227e-02,  7.64167627e-02, -3.59585238e-02,\n",
      "       -9.01536411e-04, -2.56794351e-02, -1.23072754e-02, -3.52976825e-02,\n",
      "       -2.73083572e-02,  9.15355726e-02,  6.77370136e-03, -3.72034629e-04,\n",
      "        5.43717133e-02, -4.13973865e-02,  6.91214617e-02,  2.73785597e-02,\n",
      "        5.52634595e-02, -1.45459498e-02,  6.39216372e-02, -5.55809713e-02,\n",
      "        4.12718895e-02,  1.03541799e-02, -4.71355278e-03, -4.72525450e-02,\n",
      "       -1.41146306e-02, -3.81284230e-02, -1.29423940e-02,  1.05373040e-01,\n",
      "        3.83687415e-02,  1.05954862e-01, -3.58384400e-02,  3.18246494e-02,\n",
      "       -5.08464734e-02,  6.58760216e-02,  5.89520468e-02, -1.87642536e-02,\n",
      "        7.17944441e-02,  2.27792013e-02,  3.64123582e-02, -5.44201995e-02,\n",
      "       -1.83383809e-02, -6.78449611e-02, -5.27270475e-02, -1.48505883e-02,\n",
      "       -8.53026415e-03, -2.99591085e-03,  4.71730943e-02,  4.86346545e-02,\n",
      "        1.98952581e-02,  2.57223233e-02, -4.42886297e-02, -1.83666150e-02,\n",
      "        1.20193135e-01,  1.33099204e-01, -8.05926519e-02, -4.12281679e-02,\n",
      "       -8.06737015e-02,  1.96924783e-02, -9.55491258e-03,  4.65515029e-03,\n",
      "        3.94809955e-03, -9.24588619e-02,  1.23986642e-01, -2.25442729e-02])})\n"
     ]
    }
   ],
   "source": [
    "for item in text_and_labels_test.items():\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (1/3: Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=80, random_state=42)\n",
    "\n",
    "# fit train vectors and train labels to classifier (dont get confused by this one liners)\n",
    "forest_clf.fit([item[\"vector\"] for item in text_and_labels_train.values()], [item[\"label\"] for item in text_and_labels_train.values()])\n",
    "\n",
    "# get predictions from the holy dictionary\n",
    "pred = forest_clf.predict([item[\"vector\"] for item in text_and_labels_test.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest on Test Set:  0.5188888888888888\n"
     ]
    }
   ],
   "source": [
    "# get the Accuraccy \n",
    "print(\"Accuracy for Random Forest on Test Set: \",sklearn.metrics.accuracy_score([item[\"label\"] for item in text_and_labels_test.values()], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (2/3: kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat above but with other classifiers\n",
    "neigh_clf = KNeighborsClassifier(n_neighbors=40)\n",
    "\n",
    "neigh_clf.fit([item[\"vector\"] for item in text_and_labels_train.values()], [item[\"label\"] for item in text_and_labels_train.values()])\n",
    "pred = neigh_clf.predict([item[\"vector\"] for item in text_and_labels_test.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for kNN on Test Set:  0.5455555555555556\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for kNN on Test Set: \", sklearn.metrics.accuracy_score([item[\"label\"] for item in text_and_labels_test.values()], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (3/3: Gradient Boost)\n",
    "⚠️**WARNING**⚠️ - takes long to run:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "boost_clf.fit([item[\"vector\"] for item in text_and_labels_train.values()], [item[\"label\"] for item in text_and_labels_train.values()])\n",
    "\n",
    "pred = boost_clf.predict([item[\"vector\"] for item in text_and_labels_test.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Gradient Boost on Test Set:  0.5155555555555555\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Gradient Boost on Test Set: \", sklearn.metrics.accuracy_score([item[\"label\"] for item in text_and_labels_test.values()], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-references\"></a><h2 style=\"color:rgb(0,120,170)\">References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211– 225, 2015.\n",
    "\n",
    "[2] M. Pagliardini, P. Gupta, and M. Jaggi. Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. In Proceedings of the conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
